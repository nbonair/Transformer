{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d8ca74b",
   "metadata": {
    "id": "2d8ca74b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dcaf9a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dcaf9a5",
    "outputId": "a9124824-c1f3-4a40-f48d-afa6a293684f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\n",
      "17329808/17329808 [==============================] - 15s 1us/step\n"
     ]
    }
   ],
   "source": [
    "data_path = keras.utils.get_file(\n",
    "    \"news20.tar.gz\",\n",
    "    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n",
    "    untar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b541a38",
   "metadata": {
    "id": "8b541a38"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1377be6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1377be6",
    "outputId": "7ffe98c8-ade2-4d90-d4a9-450dd1e88564"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directories: 20\n",
      "Directory names: ['comp.windows.x', 'rec.sport.hockey', 'rec.sport.baseball', 'comp.graphics', 'rec.autos', 'talk.politics.misc', 'talk.religion.misc', 'comp.os.ms-windows.misc', 'misc.forsale', 'talk.politics.mideast', 'alt.atheism', 'sci.med', 'sci.electronics', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'sci.crypt', 'sci.space', 'talk.politics.guns', 'soc.religion.christian', 'rec.motorcycles']\n",
      "Number of files in comp.graphics: 1000\n",
      "Some example filenames: ['38961', '38561', '38506', '38790', '39623']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n",
    "dirnames = os.listdir(data_dir)\n",
    "print(\"Number of directories:\", len(dirnames))\n",
    "print(\"Directory names:\", dirnames)\n",
    "\n",
    "fnames = os.listdir(data_dir / \"comp.graphics\")\n",
    "print(\"Number of files in comp.graphics:\", len(fnames))\n",
    "print(\"Some example filenames:\", fnames[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d5d788",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78d5d788",
    "outputId": "834785c7-c05b-4337-c893-7bcfe369447a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newsgroups: comp.graphics\n",
      "Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!agate!dog.ee.lbl.gov!network.ucsd.edu!usc!rpi!nason110.its.rpi.edu!mabusj\n",
      "From: mabusj@nason110.its.rpi.edu (Jasen M. Mabus)\n",
      "Subject: Looking for Brain in CAD\n",
      "Message-ID: <c285m+p@rpi.edu>\n",
      "Nntp-Posting-Host: nason110.its.rpi.edu\n",
      "Reply-To: mabusj@rpi.edu\n",
      "Organization: Rensselaer Polytechnic Institute, Troy, NY.\n",
      "Date: Thu, 29 Apr 1993 23:27:20 GMT\n",
      "Lines: 7\n",
      "\n",
      "Jasen Mabus\n",
      "RPI student\n",
      "\n",
      "\tI am looking for a hman brain in any CAD (.dxf,.cad,.iges,.cgm,etc.) or picture (.gif,.jpg,.ras,etc.) format for an animation demonstration. If any has or knows of a location please reply by e-mail to mabusj@rpi.edu.\n",
      "\n",
      "Thank you in advance,\n",
      "Jasen Mabus  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(open(data_dir / \"comp.graphics\" / \"38987\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e7155",
   "metadata": {
    "id": "3c2e7155"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ecde906",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ecde906",
    "outputId": "ad9924d5-26ef-4cd7-8057-046d2f631eb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing alt.atheism, 1000 files found\n",
      "Processing comp.graphics, 1000 files found\n",
      "Processing comp.os.ms-windows.misc, 1000 files found\n",
      "Processing comp.sys.ibm.pc.hardware, 1000 files found\n",
      "Processing comp.sys.mac.hardware, 1000 files found\n",
      "Processing comp.windows.x, 1000 files found\n",
      "Processing misc.forsale, 1000 files found\n",
      "Processing rec.autos, 1000 files found\n",
      "Processing rec.motorcycles, 1000 files found\n",
      "Processing rec.sport.baseball, 1000 files found\n",
      "Processing rec.sport.hockey, 1000 files found\n",
      "Processing sci.crypt, 1000 files found\n",
      "Processing sci.electronics, 1000 files found\n",
      "Processing sci.med, 1000 files found\n",
      "Processing sci.space, 1000 files found\n",
      "Processing soc.religion.christian, 997 files found\n",
      "Processing talk.politics.guns, 1000 files found\n",
      "Processing talk.politics.mideast, 1000 files found\n",
      "Processing talk.politics.misc, 1000 files found\n",
      "Processing talk.religion.misc, 1000 files found\n",
      "Classes: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Number of samples: 19997\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "labels = []\n",
    "class_names = []\n",
    "class_index = 0\n",
    "for dirname in sorted(os.listdir(data_dir)):\n",
    "    class_names.append(dirname)\n",
    "    dirpath = data_dir / dirname\n",
    "    fnames = os.listdir(dirpath)\n",
    "    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n",
    "    for fname in fnames:\n",
    "        fpath = dirpath / fname\n",
    "        f = open(fpath, encoding=\"latin-1\")\n",
    "        content = f.read()\n",
    "        lines = content.split(\"\\n\")\n",
    "        lines = lines[10:]\n",
    "        content = \"\\n\".join(lines)\n",
    "        samples.append(content)\n",
    "        labels.append(class_index)\n",
    "    class_index += 1\n",
    "\n",
    "print(\"Classes:\", class_names)\n",
    "print(\"Number of samples:\", len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec3f37ff",
   "metadata": {
    "id": "ec3f37ff"
   },
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "seed = 12345\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(samples)\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(labels)\n",
    "\n",
    "# Extract a training & validation split\n",
    "validation_split = 0.2\n",
    "num_validation_samples = int(validation_split * len(samples))\n",
    "\n",
    "train_samples = samples[:-num_validation_samples]\n",
    "val_samples   = samples[-num_validation_samples:]\n",
    "train_labels  = labels[:-num_validation_samples]\n",
    "val_labels    = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d750f78",
   "metadata": {
    "id": "1d750f78"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55b80a95",
   "metadata": {
    "id": "55b80a95"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6da583c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6da583c6",
    "outputId": "1d1db474-726a-4804-e63f-b448d60f196f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'to', 'of', 'a', 'and', 'in', 'is', 'i']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81fb4827",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81fb4827",
    "outputId": "f66cdb5d-7582-41f9-aaaf-90064d5264a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2, 3762, 1723,   15,    2, 5624])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = vectorizer([[\"the cat sat on the mat\"]])\n",
    "output.numpy()[0, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77193d7",
   "metadata": {
    "id": "e77193d7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6df11735",
   "metadata": {
    "id": "6df11735"
   },
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e62c2b87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e62c2b87",
    "outputId": "80c095b2-cc58-4678-dad1-5b8dd8254284"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3762, 1723, 15, 2, 5624]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "[word_index[w] for w in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b365d",
   "metadata": {
    "id": "6c5b365d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73bf0dae",
   "metadata": {
    "id": "73bf0dae"
   },
   "outputs": [],
   "source": [
    "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c382261c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c382261c",
    "outputId": "da1d1cbe-c964-4067-d89e-c7b9c88a0602"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2496,  1609,  2650,  1960,   181,   420,  1307,  1747,   492,\n",
       "         266,   130,   690,   242,   156,  9739,     1,  1699,   800,\n",
       "           5,  8000,  9733,  7164,  4604,  8306, 19592,  3240,  1033,\n",
       "         877,    64,   417,   106,   417,   106,   106,   106,   106,\n",
       "         135,    32,  2266,   229,   336,  2097,  9105,    79,   245,\n",
       "         155,   114,   135,   106,    79,   106,   106,   506,  2831,\n",
       "        6047,   314,  3373,  6072,   114,   265,   203,    64,    79,\n",
       "         106,   106,   106,   203,  9072,  2713,  6047,  2620,  3793,\n",
       "        6726,   114,   265,   135,   114,    64,    64,   106,   106,\n",
       "         135,    32,  2274,   229,  2220,  3455,  3911,   114,   265,\n",
       "         114,   135,    79,    64,   106,   106,   601,    32,  2013,\n",
       "        6047,   449,     1,  7568,   114,   265,    79,   155,    64,\n",
       "         106,   106,   106,   336,    32,  2325,   229,   496,  1476,\n",
       "        4515,   114,   265,    64,   203,   106,   106,   106,   106,\n",
       "         106,    32,  2255,   464,  3147,   507,  8072,   114,   265,\n",
       "          64,   203,   106,   106,   106,   106,   106,   506,  2713,\n",
       "         229,   314,  1344,  6328,   417,   203,   135,    79,   106,\n",
       "         106,    79,   106,   135,    32,  2651,   229,   242,  1496,\n",
       "       10695,   417,   203,   114,   114,   106,   106,    64,   106,\n",
       "          79,  9072,  1342,  6047,   206, 13967,  6447,   417,   203,\n",
       "         114,   114,    64,   106,    64,   106,   106,  9072,  2713,\n",
       "        6820,   511, 11147, 13474,   417,   203,   114,   114,    64,\n",
       "         106,    64])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441486f2",
   "metadata": {
    "id": "441486f2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ee0bda0",
   "metadata": {
    "id": "2ee0bda0"
   },
   "outputs": [],
   "source": [
    "# tf.data.Dataset\n",
    "\n",
    "def create_dataset(x, y):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.batch(100)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_ds = create_dataset(x_train, y_train)\n",
    "test_ds = create_dataset(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82ed5224",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82ed5224",
    "outputId": "716b8981-2527-4614-8043-c38a3bb8644d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 2496  1609  2650  1960   181   420  1307  1747   492   266   130   690\n",
      "   242   156  9739     1  1699   800     5  8000  9733  7164  4604  8306\n",
      " 19592  3240  1033   877    64   417   106   417   106   106   106   106\n",
      "   135    32  2266   229   336  2097  9105    79   245   155   114   135\n",
      "   106    79   106   106   506  2831  6047   314  3373  6072   114   265\n",
      "   203    64    79   106   106   106   203  9072  2713  6047  2620  3793\n",
      "  6726   114   265   135   114    64    64   106   106   135    32  2274\n",
      "   229  2220  3455  3911   114   265   114   135    79    64   106   106\n",
      "   601    32  2013  6047   449     1  7568   114   265    79   155    64\n",
      "   106   106   106   336    32  2325   229   496  1476  4515   114   265\n",
      "    64   203   106   106   106   106   106    32  2255   464  3147   507\n",
      "  8072   114   265    64   203   106   106   106   106   106   506  2713\n",
      "   229   314  1344  6328   417   203   135    79   106   106    79   106\n",
      "   135    32  2651   229   242  1496 10695   417   203   114   114   106\n",
      "   106    64   106    79  9072  1342  6047   206 13967  6447   417   203\n",
      "   114   114    64   106    64   106   106  9072  2713  6820   511 11147\n",
      " 13474   417   203   114   114    64   106    64], shape=(200,), dtype=int64)\n",
      "tf.Tensor(10, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_ds:\n",
    "    print(x[0])\n",
    "    print(y[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OZ9a1SOC7kFE",
   "metadata": {
    "id": "OZ9a1SOC7kFE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2pTTtN264ioY",
   "metadata": {
    "id": "2pTTtN264ioY"
   },
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "nlv8EeOC4iq6",
   "metadata": {
    "id": "nlv8EeOC4iq6"
   },
   "outputs": [],
   "source": [
    "# Two seperate embedding layers, one for tokens, one for token index (positions)\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ps2YVjwj4itW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ps2YVjwj4itW",
    "outputId": "58edbc84-4132-44de-f14f-ab04719f6122"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 200, 128)         2585600   \n",
      " g_2 (TokenAndPositionEmbedd                                     \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_block_2 (Transf  (None, 200, 128)         429184    \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " global_average_pooling1d_2   (None, 128)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 20)                660       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,019,572\n",
      "Trainable params: 3,019,572\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "vocab_size   = 20000\n",
    "sequence_len = 200\n",
    "\n",
    "embed_dim = 128  # Embedding size for each token\n",
    "num_heads = 6    # Number of attention heads\n",
    "ff_dim = 128     # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(sequence_len,))\n",
    "embedding_layer = TokenAndPositionEmbedding(sequence_len, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(20, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "-o-cpf6t4ivq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-o-cpf6t4ivq",
    "outputId": "3ea9ed98-f9fb-4f85-bbc3-e9bc0f654f01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "160/160 [==============================] - 27s 140ms/step - loss: 2.6065 - accuracy: 0.1607 - val_loss: 1.5984 - val_accuracy: 0.4609\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 10s 64ms/step - loss: 1.0438 - accuracy: 0.6688 - val_loss: 0.7367 - val_accuracy: 0.7757\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 10s 65ms/step - loss: 0.4432 - accuracy: 0.8673 - val_loss: 0.7527 - val_accuracy: 0.8010\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 11s 67ms/step - loss: 0.2972 - accuracy: 0.9081 - val_loss: 0.8318 - val_accuracy: 0.7912\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 10s 64ms/step - loss: 0.2097 - accuracy: 0.9305 - val_loss: 1.0326 - val_accuracy: 0.7854\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 10s 64ms/step - loss: 0.1531 - accuracy: 0.9486 - val_loss: 0.9979 - val_accuracy: 0.7962\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 10s 65ms/step - loss: 0.1310 - accuracy: 0.9557 - val_loss: 1.1091 - val_accuracy: 0.7934\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 10s 64ms/step - loss: 0.1144 - accuracy: 0.9581 - val_loss: 1.1910 - val_accuracy: 0.7887\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 10s 64ms/step - loss: 0.1018 - accuracy: 0.9614 - val_loss: 1.2433 - val_accuracy: 0.7914\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 10s 64ms/step - loss: 0.0943 - accuracy: 0.9630 - val_loss: 1.2674 - val_accuracy: 0.7877\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=opt, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_ds, batch_size=32, epochs=10, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asqML3Rv4ix9",
   "metadata": {
    "id": "asqML3Rv4ix9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iM8jUQo-4i0E",
   "metadata": {
    "id": "iM8jUQo-4i0E"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
